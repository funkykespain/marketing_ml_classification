{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. ML - Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv('../data/processed/campana_marketing.csv', parse_dates=['Dt_Customer'])\n",
    "\n",
    "# Convertir la columna 'Income' de float64 a int64\n",
    "df['Income'] = df['Income'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuestra variable objetivo será `AcceptedCmp`: 0 no se ha aceptado ninguna campaña y 1 se ha aceptado al menos 1 de las 5 campañas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AcceptedCmp            1.000000\n",
       "AcceptedCmp4           0.553782\n",
       "AcceptedCmp3           0.551957\n",
       "AcceptedCmp5           0.548292\n",
       "AcceptedCmp1           0.510624\n",
       "MntWines               0.465034\n",
       "Spent                  0.412466\n",
       "Response               0.367401\n",
       "Income                 0.315024\n",
       "NumCatalogPurchases    0.313291\n",
       "MntMeatProducts        0.274626\n",
       "AcceptedCmp2           0.229464\n",
       "NumWebPurchases        0.213228\n",
       "MntGoldProds           0.190782\n",
       "NumStorePurchases      0.186765\n",
       "MntFishProducts        0.160046\n",
       "MntSweetProducts       0.159584\n",
       "MntFruits              0.126553\n",
       "Education              0.047823\n",
       "Age                    0.030778\n",
       "Marital_Status        -0.000198\n",
       "Days                  -0.013751\n",
       "Seniority             -0.013751\n",
       "Recency               -0.017745\n",
       "Complain              -0.027016\n",
       "Year_Birth            -0.030778\n",
       "ID                    -0.041206\n",
       "NumDealsPurchases     -0.086570\n",
       "Teenhome              -0.099415\n",
       "NumWebVisitsMonth     -0.125986\n",
       "Kidhome               -0.203024\n",
       "Child_Home            -0.228974\n",
       "Name: AcceptedCmp, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Correlación con el Target\n",
    "correlation_matrix = df.corr(numeric_only=True)\n",
    "correlation_with_target = correlation_matrix['AcceptedCmp'].sort_values(ascending=False)\n",
    "correlation_with_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de variables X e Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Determinamos que las variables con una correlación entre 0.2 y -0.2 quedarán excluidas de nuestro entrenamiento, ya que se encuentran entorno al 0 y parecen que no afectan a la variable objetivo y así aligeramos el proceso de entrenamiento.\n",
    "2. Usando una **list comprehension** eliminamos las características `AcceptedCmp1, AcceptedCmp2, AcceptedCmp3, AcceptedCmp4 y AcceptedCmp5`, ya que son subproducto de AcceptedCmp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características seleccionadas: ['MntWines', 'Spent', 'Response', 'Income', 'NumCatalogPurchases', 'MntMeatProducts', 'NumWebPurchases', 'Kidhome', 'Child_Home']\n"
     ]
    }
   ],
   "source": [
    "# Filtrar correlaciones fuera del rango [-0.2, 0.2]\n",
    "filtered_correlations = correlation_with_target[\n",
    "    correlation_with_target.abs() > 0.2\n",
    "].sort_values(ascending=False)\n",
    "\n",
    "# Lista final de features, excluyendo las variables derivadas de 'AcceptedCmp'\n",
    "excluded_features = ['AcceptedCmp', 'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "features = [f for f in filtered_correlations.index if f not in excluded_features]\n",
    "\n",
    "print(\"Características seleccionadas:\", features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features]\n",
    "y = df['AcceptedCmp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División de datos\n",
    "Cogeremos el 20% para test y el 80% para training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamiento del Desbalance de Clases\n",
    "Como vimos en [**EDA**](04_explore_data.ipynb), al final del análisis no gráfico, `AcceptedCmp` tiene la clase está desbalanceada.\n",
    "En este escenario, la clase minoritaria (en nuestro caso, la clase 1) está significativamente menos representada que la clase mayoritaria (la clase 0). Esto puede generar varios problemas al entrenar un modelo de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebalancear clases\n",
    "Optamos por hacer un Submuestreo (**Undersampling**), reduciendo el número de ejemplos de la clase mayoritaria.\n",
    "\n",
    "Dada la significativa disparidad en la representación de las clases en nuestro conjunto de datos, con una marcada predominancia de la clase mayoritaria (clase 0), se ha optado por aplicar la técnica de submuestreo (undersampling). El objetivo principal de esta estrategia es reducir el número de instancias de la clase mayoritaria para mitigar el sesgo del modelo hacia esta clase y mejorar su capacidad para aprender y predecir la clase minoritaria (clase 1) de manera más efectiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar undersampling para balancear las clases\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de modelos\n",
    "Para el Desbalance de Clases, ajustamos pesos en el modelo (más fácil y eficiente) con `class_weight='balanced`. \n",
    "\n",
    "Como los modelos KNN y Neural Network no son compatibles para que se ajusten, aquí sí que aplicaremos el undersampling que se ha hecho previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos con ajuste de pesos para modelos compatibles\n",
    "models = {\n",
    "    'Logistic Regression': (LogisticRegression(max_iter=500, class_weight='balanced'), X_train, y_train),\n",
    "    'KNN': (KNeighborsClassifier(), X_resampled, y_resampled),\n",
    "    'Random Forest': (RandomForestClassifier(class_weight='balanced'), X_train, y_train),\n",
    "    'SVM': (SVC(probability=True, class_weight='balanced'), X_train, y_train),\n",
    "    'Neural Network': (MLPClassifier(), X_resampled, y_resampled)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros para GridSearchCV\n",
    "param_grid = {\n",
    "    'Logistic Regression': {'C': [0.01, 0.1, 1, 10, 100]},\n",
    "    'KNN': {'n_neighbors': [3, 5, 7, 9, 11]},\n",
    "    'Random Forest': {'n_estimators': [50, 100, 200], 'max_features': ['sqrt', 'log2'], 'max_depth': [3, 5, 10]},\n",
    "    'SVM': {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]},\n",
    "    'Neural Network': {\n",
    "        'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.05],\n",
    "        'learning_rate': ['constant','adaptive']\n",
    "    }\n",
    "}\n",
    "\n",
    "results = {}\n",
    "best_models = {}\n",
    "\n",
    "for model_name, (model, X_data, y_data) in models.items():\n",
    "    print(f\"\\nEntrenando {model_name}...\")\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid[model_name], scoring='roc_auc', n_jobs=-1, cv=5)\n",
    "    grid_search.fit(X_data, y_data)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    results[model_name] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1 Score': f1_score(y_test, y_pred),\n",
    "        'ROC AUC': roc_auc_score(y_test, y_pred_proba),\n",
    "        'Best Params': best_params\n",
    "    }\n",
    "    best_models[model_name] = best_model\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df[['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']] = results_df[['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']].astype(float)\n",
    "\n",
    "print(\"\\nResultados de los modelos:\")\n",
    "print(results_df)\n",
    "\n",
    "# Determinar el mejor modelo basado en diferentes métricas\n",
    "best_metrics = {metric: results_df[metric].idxmax() for metric in ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']}\n",
    "\n",
    "print(\"\\n🔹 Mejores modelos por métrica:\")\n",
    "for metric, model in best_metrics.items():\n",
    "    print(f\"Mejor modelo basado en {metric}: {model} - {results_df.at[model, 'Best Params']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marketing-ml-a6pcPZMr-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
